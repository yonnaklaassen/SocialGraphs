{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Lab  to Lecture: \n",
    "\n",
    "## Analyzing the Connection Between Professors‚Äô Research and Course Content‚Äã üë©‚Äçüè´\n",
    "\n",
    "**Authors**:\n",
    "- Erik Wold Riise, s194633‚Äã\n",
    "- Lukas Rasocha, s233498‚Äã\n",
    "- Zou Yong Nan Klaassen, s230351"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Intro Image](./assets/intro.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Image Prompt: minimalistic network visualization with two nodes: one representing a professor and the other a course they teach, connected by a single edge*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Overview üìä\n",
    "\n",
    "We collected data from 3 separate sources. The first source is the dtu course base from which we gathered information about the courses and the professors teaching them (including the grades, ratings, descriptions, prerequisites, etc.). The second source is the dtu orbit database, which contains information about the professors and their research from which we scraped all the necessary fields for each professor. The third source is a platform called scholia, which contains additional information about the professors and their research. We used this data to create a bipartite bipartite bipartite bipartite bipartite bipartite bipartite bipartite bipartite network of professors and courses, where the nodes are the professors and the courses they teach, and the edges are the connections between them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Overview ‚úçÔ∏è\n",
    "\n",
    "This project investigates the alignment between professors‚Äô research areas and the courses they teach through the angle of network analysis and natural language processing (NLP).\n",
    "We plan to construct a bipartite graph of professors and courses, and analyze the structural and thematic patterns in teaching and research connections.\n",
    "\n",
    "The central research question steering the project is:\n",
    "_\"How well do professors‚Äô research areas align with the content and objectives of the courses they teach, and how does this alignment vary across disciplines?\"_\n",
    "\n",
    "To complement this, we also examine:\n",
    "_\"Does the alignment between professors‚Äô research and the courses they teach influence student satisfaction and performance (grades)?\"_\n",
    "\n",
    "Using NLP techniques, we analyze course descriptions and research topics to measure alignment, and we relate these findings to course evaluations and grades. Additionally, network analysis methods, such as community detection and centrality measures, will be applied to uncover interdisciplinary trends and the influence of professors within the academic network.\n",
    "\n",
    "By this we hope to shed light on how expertise and teaching intersect, and how does that impact educational outcomes in a broader sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Directions\n",
    "\n",
    "- Does the alignment between professors‚Äô research and the courses they teach influence student satisfaction and performance (grades)?\n",
    "\n",
    "- How well do professors‚Äô research areas align with the content and objectives of the courses they teach, and how does this alignment vary across disciplines?\n",
    "\n",
    "- In a situation where a professor quits, how can we find a suitable replacement based on the research areas of the professor and the courses they teach?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from scholia import query as scholia_query\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Course Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/course_df.csv'\n",
    "course_df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DTU Orbit Scraper class + Scholia API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DTUOrbitScraper:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://orbit.dtu.dk/en/persons/\"\n",
    "        self.endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "    def search_person(self, name):\n",
    "        \"\"\"Search for the person and get the URL to their profile.\"\"\"\n",
    "        search_url = f\"{self.base_url}?search={name.replace(' ', '+')}&isCopyPasteSearch=false\"\n",
    "        response = requests.get(search_url)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            raise Exception(\"Failed to fetch search results\")\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        # Find the first profile link (assuming it's the first result)\n",
    "        profile_link = soup.find(\"h3\", class_=\"title\").find(\"a\", href=True)\n",
    "        \n",
    "        if profile_link:\n",
    "            return profile_link['href']\n",
    "        else:\n",
    "            raise Exception(\"Profile link not found\")\n",
    "\n",
    "    def get_topic_info(self, topic_url):\n",
    "        \"\"\"Scrape the description for a topic from its Wikidata page.\"\"\"\n",
    "        response = requests.get(topic_url)\n",
    "        if response.status_code != 200:\n",
    "            return \"Description not found\"\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        description = soup.find(\"div\", class_=\"wikibase-entitytermsview-heading-description\")\n",
    "        return description.text.strip() if description else \"Description not found\"\n",
    "\n",
    "    def get_scholia_topics(self, qs):\n",
    "        \"\"\"Get topics and scores from Scholia using SPARQL.\"\"\"\n",
    "        query = f\"\"\"PREFIX target: <http://www.wikidata.org/entity/{qs}>\n",
    "        SELECT ?score ?topic ?topicLabel\n",
    "        WITH {{\n",
    "            SELECT (SUM(?score_) AS ?score) ?topic WHERE {{\n",
    "                {{ target: wdt:P101 ?topic . BIND(20 AS ?score_) }}\n",
    "                UNION {{ SELECT (3 AS ?score_) ?topic WHERE {{ ?work wdt:P50 target: ; wdt:P921 ?topic . }} }}\n",
    "                UNION {{ SELECT (1 AS ?score_) ?topic WHERE {{ ?work wdt:P50 target: . ?citing_work wdt:P2860 ?work . ?citing_work wdt:P921 ?topic . }} }}\n",
    "            }} GROUP BY ?topic\n",
    "        }} AS %results \n",
    "        WHERE {{\n",
    "            INCLUDE %results\n",
    "            SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}\n",
    "        }}\n",
    "        ORDER BY DESC(?score)\n",
    "        LIMIT 200\"\"\"\n",
    "        \n",
    "        sparql = SPARQLWrapper(self.endpoint_url)\n",
    "        sparql.setQuery(query)\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        results = sparql.query().convert()\n",
    "        \n",
    "        topics = [] \n",
    "\n",
    "        for result in results[\"results\"][\"bindings\"]:\n",
    "            topic_url = result[\"topic\"][\"value\"]\n",
    "            topic_label = result[\"topicLabel\"][\"value\"]\n",
    "            score = int(result[\"score\"][\"value\"])\n",
    "            #info = self.get_topic_info(topic_url)\n",
    "            #topics[topic_label] = {\"score\": score, \"info\": info}\n",
    "            topics.append({\"topic\":topic_label, \"score\": score, \"topic_url\": topic_url})\n",
    "        return topics\n",
    "\n",
    "    def get_profile_info(self, name):\n",
    "        \"\"\"Retrieve profile information given a person's name.\"\"\"\n",
    "        full_profile_url = self.search_person(name)\n",
    "        response = requests.get(full_profile_url)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            raise Exception(\"Failed to fetch profile page\")\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Extract profile information\n",
    "        profile_info = {}\n",
    "        \n",
    "        # Get Profile Description\n",
    "        profile_header = soup.find(\"h3\", string=\"Profile\")\n",
    "        profile_section = profile_header.find_next(\"p\") if profile_header else None\n",
    "        profile_info[\"Profile_desc\"] = profile_section.get_text(strip=True) if profile_section else \"None\"\n",
    "        \n",
    "        # Get Keywords\n",
    "        keywords_section = soup.find(\"div\", class_=\"keyword-group\")\n",
    "        if keywords_section:\n",
    "            keywords = [keyword.get_text(strip=True) for keyword in keywords_section.find_all(\"li\", class_=\"userdefined-keyword\")]\n",
    "            profile_info[\"Keywords\"] = keywords\n",
    "        else:\n",
    "            profile_info[\"Keywords\"] = []\n",
    "\n",
    "        # Get Fingerprint (Concepts, Thesauri, Values)\n",
    "        fingerprints = []\n",
    "        fingerprint_section = soup.find(\"div\", class_=\"person-top-concepts\")\n",
    "        if fingerprint_section:\n",
    "            fingerprint_items = fingerprint_section.find_all(\"li\", class_=\"concept-badge-large-container\")\n",
    "            for item in fingerprint_items:\n",
    "                concept = item.find(\"span\", class_=\"concept\").get_text(strip=True) if item.find(\"span\", class_=\"concept\") else \"N/A\"\n",
    "                thesauri = item.find(\"span\", class_=\"thesauri\").get_text(strip=True) if item.find(\"span\", class_=\"thesauri\") else \"N/A\"\n",
    "                value = item.find(\"span\", class_=\"value sr-only\").get_text(strip=True) if item.find(\"span\", class_=\"value sr-only\") else \"N/A\"\n",
    "                fingerprints.append({\n",
    "                    \"Concept\": concept,\n",
    "                    \"Thesauri\": thesauri,\n",
    "                    \"Value\": value\n",
    "                })\n",
    "        profile_info[\"Fingerprint\"] = fingerprints\n",
    "\n",
    "        # Extract ORCID\n",
    "        orcid_section = soup.find(\"div\", class_=\"rendering_person_personorcidrendererportal\")\n",
    "        if orcid_section:\n",
    "            orcid_link = orcid_section.find(\"a\", href=True)\n",
    "            profile_info[\"ORCID\"] = orcid_link[\"href\"] if orcid_link else \"Not found\"\n",
    "            if orcid_link:\n",
    "                orcid_id = orcid_link[\"href\"].split(\"/\")[-1]\n",
    "                profile_info[\"QS\"] = scholia_query.orcid_to_qs(orcid_id)\n",
    "                # Retrieve Scholia topics if QS exists\n",
    "                if len(profile_info[\"QS\"]) == 1:\n",
    "                    profile_info[\"scholia_topics\"] = self.get_scholia_topics(profile_info[\"QS\"][0])\n",
    "                else:\n",
    "                    profile_info[\"scholia_topics\"] = {}\n",
    "        else:\n",
    "            profile_info[\"ORCID\"] = \"Not found\"\n",
    "            profile_info[\"QS\"] = \"Not found\"\n",
    "            profile_info[\"scholia_topics\"] = {}\n",
    "\n",
    "        return profile_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Professors information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data/all_professors.json already exists. Skipping scraping.\n"
     ]
    }
   ],
   "source": [
    "scraper = DTUOrbitScraper()\n",
    "\n",
    "professor_columns = [\n",
    "    \"MAIN_RESPONSIBLE_NAME\", \"CO_RESPONSIBLE_1_NAME\",\n",
    "    \"CO_RESPONSIBLE_2_NAME\", \"CO_RESPONSIBLE_3_NAME\", \"CO_RESPONSIBLE_4_NAME\"\n",
    "]\n",
    "\n",
    "# Extract unique professors from the dataset\n",
    "def extract_professors(dataframe, professor_columns):\n",
    "    professors = set()\n",
    "    for col in professor_columns:\n",
    "        professors.update(dataframe[col].dropna().unique())\n",
    "    return list(professors)\n",
    "\n",
    "\n",
    "def scrape_professor_data(professors, output_file):\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"File {output_file} already exists. Skipping scraping.\")\n",
    "        return\n",
    "\n",
    "    all_data = {}\n",
    "\n",
    "    for professor in tqdm(professors, desc=\"Scraping Professors\"):\n",
    "        if professor in all_data:\n",
    "            print(f\"Skipping {professor} as it already exists in the JSON file.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            profile_info = scraper.get_profile_info(professor)\n",
    "            all_data[professor] = profile_info  # Add to dictionary\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to scrape data for {professor}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(all_data, f, indent=4)\n",
    "    \n",
    "    print(f\"Saved all professor data to {output_file}\")\n",
    "\n",
    "\n",
    "output_file = \"data/all_professors.json\"\n",
    "professors = extract_professors(course_df, professor_columns)\n",
    "scrape_professor_data(professors, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Professors data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/all_professors.json\", \"r\") as f:\n",
    "    professors_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Preprocessing\n",
    "Write in more detail about your choices in data cleaning and preprocessing\n",
    "Did you do analyses / calculate statistics that didn't make it to the main text, put them here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaratory Data Analysis of the Network\n",
    "\n",
    "- Lecture 1, 2, 3, 4, 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Bipartite Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bipartite graph created with 2767 nodes and 4802 edges.\n"
     ]
    }
   ],
   "source": [
    "B = nx.Graph()\n",
    "\n",
    "professors = extract_professors(course_df, professor_columns)\n",
    "courses = course_df[\"COURSE\"].unique()\n",
    "\n",
    "B.add_nodes_from(professors, bipartite=0, type=\"Professor\")  # Professors\n",
    "B.add_nodes_from(courses, bipartite=1, type=\"Course\")  # Courses\n",
    "\n",
    "# Add edges based on professor-course relationships\n",
    "for _, row in course_df.iterrows():\n",
    "    course = row[\"COURSE\"]\n",
    "    for col in professor_columns:\n",
    "        professor = row[col]\n",
    "        if pd.notna(professor):  \n",
    "            B.add_edge(professor, course)\n",
    "\n",
    "# Add metadata to nodes\n",
    "for professor in professors:\n",
    "    if professor in professors_data:\n",
    "        # Add professor metadata (e.g., research areas)\n",
    "        B.nodes[professor][\"Profile_desc\"] = professors_data[professor].get(\"Profile_desc\", None)\n",
    "        B.nodes[professor][\"Keywords\"] = professors_data[professor].get(\"Keywords\", [])\n",
    "        B.nodes[professor][\"Fingerprint\"] = professors_data[professor].get(\"Fingerprint\", [])\n",
    "        B.nodes[professor][\"scholia_topics\"] = professors_data[professor].get(\"scholia_topics\", [])\n",
    "\n",
    "for _, row in course_df.iterrows():\n",
    "    course = row[\"COURSE\"]\n",
    "    # Add course metadata (e.g., description and objectives)\n",
    "    B.nodes[course][\"COURSE_DESCRIPTION\"] = row.get(\"COURSE_DESCRIPTION\", \"\")\n",
    "    B.nodes[course][\"LEARNING_OBJECTIVES\"] = row.get(\"LEARNING_OBJECTIVES\", \"\")\n",
    "\n",
    "print(f\"Bipartite graph created with {B.number_of_nodes()} nodes and {B.number_of_edges()} edges.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data analysis of Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lecture 5, 6, 7, 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Analysis and our Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scholia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
