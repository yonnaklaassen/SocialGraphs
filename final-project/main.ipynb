{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Lab  to Lecture: \n",
    "\n",
    "## Analyzing the Connection Between Professors‚Äô Research and Course Content‚Äã üë©‚Äçüè´\n",
    "\n",
    "**Authors**:\n",
    "- Erik Wold Riise, s194633‚Äã\n",
    "- Lukas Rasocha, s233498‚Äã\n",
    "- Zou Yong Nan Klaassen, s230351"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"assets/intro.png\" width=\"1000\"  /></center>\n",
    "\n",
    "*Image Prompt: minimalistic network visualization with two nodes: one representing a professor and the other a course they teach, connected by a single edge*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Overview\n",
    "This project investigates the alignment between professors‚Äô research areas and the courses they teach through the angle of network analysis and natural language processing (NLP). By constructing a bipartite graph of professors and courses, we aim to explore structural and thematic patterns in teaching and research connections.\n",
    "\n",
    "The central research question steering the project is:\n",
    "*\"How well do professors‚Äô research areas align with the content and objectives of the courses they teach, and how does this alignment vary across disciplines?\"*\n",
    "\n",
    "To complement this, we also examine:\n",
    "*\"Does the alignment between professors‚Äô research and the courses they teach influence student satisfaction and performance (grades)?\"*\n",
    "\n",
    "Using NLP techniques, we analyze course descriptions and research topics to measure alignment, and we relate these findings to course evaluations and grades. Additionally, network analysis methods, such as community detection and centrality measures, are applied to uncover interdisciplinary trends and the influence of professors within the academic network. \n",
    "\n",
    "By this we hope to shed light on how expertise and teaching intersect, and how does that impact educational outcomes in a broader sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from scholia import query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/course_df.csv'\n",
    "course_df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DTU Orbit Scraper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTUOrbitScraper:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://orbit.dtu.dk/en/persons/\"\n",
    "        \n",
    "    def search_person(self, name):\n",
    "        \"\"\"Search for the person and get the URL to their profile.\"\"\"\n",
    "        search_url = f\"{self.base_url}?search={name.replace(' ', '+')}&isCopyPasteSearch=false\"\n",
    "        response = requests.get(search_url)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            raise Exception(\"Failed to fetch search results\")\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        # Find the first profile link (assuming it's the first result)\n",
    "        profile_link = soup.find(\"h3\", class_=\"title\").find(\"a\", href=True)\n",
    "        \n",
    "        if profile_link:\n",
    "            return profile_link['href']\n",
    "        else:\n",
    "            raise Exception(\"Profile link not found\")\n",
    "\n",
    "    def get_profile_info(self, name):\n",
    "        \"\"\"Retrieve profile information given a person's name.\"\"\"\n",
    "        full_profile_url = self.search_person(name)\n",
    "        response = requests.get(full_profile_url)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            raise Exception(\"Failed to fetch profile page\")\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Extract profile information\n",
    "        profile_info = {}\n",
    "        \n",
    "        # Get Profile Description\n",
    "        profile_header = soup.find(\"h3\", string=\"Profile\")\n",
    "        profile_section = profile_header.find_next(\"p\") if profile_header else None\n",
    "        profile_info[\"Profile_desc\"] = profile_section.get_text(strip=True) if profile_section else \"None\"\n",
    "        \n",
    "        # Get Keywords\n",
    "        keywords_section = soup.find(\"div\", class_=\"keyword-group\")\n",
    "        if keywords_section:\n",
    "            keywords = [keyword.get_text(strip=True) for keyword in keywords_section.find_all(\"li\", class_=\"userdefined-keyword\")]\n",
    "            profile_info[\"Keywords\"] = keywords\n",
    "        else:\n",
    "            profile_info[\"Keywords\"] = []\n",
    "\n",
    "        # Get Fingerprint (Concepts, Thesauri, Values)\n",
    "        fingerprints = []\n",
    "        fingerprint_section = soup.find(\"div\", class_=\"person-top-concepts\")\n",
    "        if fingerprint_section:\n",
    "            fingerprint_items = fingerprint_section.find_all(\"li\", class_=\"concept-badge-large-container\")\n",
    "            for item in fingerprint_items:\n",
    "                concept = item.find(\"span\", class_=\"concept\").get_text(strip=True) if item.find(\"span\", class_=\"concept\") else \"N/A\"\n",
    "                thesauri = item.find(\"span\", class_=\"thesauri\").get_text(strip=True) if item.find(\"span\", class_=\"thesauri\") else \"N/A\"\n",
    "                value = item.find(\"span\", class_=\"value sr-only\").get_text(strip=True) if item.find(\"span\", class_=\"value sr-only\") else \"N/A\"\n",
    "                fingerprints.append({\n",
    "                    \"Concept\": concept,\n",
    "                    \"Thesauri\": thesauri,\n",
    "                    \"Value\": value\n",
    "                })\n",
    "        profile_info[\"Fingerprint\"] = fingerprints\n",
    "\n",
    "        # Get ORCID\n",
    "        orcid_section = soup.find(\"div\", class_=\"rendering_person_personorcidrendererportal\")\n",
    "        if orcid_section:\n",
    "            orcid_link = orcid_section.find(\"a\", href=True)\n",
    "            profile_info[\"ORCID\"] = orcid_link[\"href\"] if orcid_link else \"Not found\"\n",
    "\n",
    "            profile_info[\"QS\"] = query.orcid_to_qs(orcid_link[\"href\"].split(\"/\")[-1]) if orcid_link else \"Not found\"\n",
    "        else:\n",
    "            profile_info[\"ORCID\"] = \"Not found\"\n",
    "        return profile_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Professors information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Professors:   7%|‚ñã         | 75/1063 [01:34<18:31,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape data for Jacqueline Eve Stenson: 'NoneType' object has no attribute 'find'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Professors:  11%|‚ñà         | 112/1063 [02:26<18:29,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape data for Carolyn Rutherford: 'NoneType' object has no attribute 'find'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Professors:  25%|‚ñà‚ñà‚ñç       | 264/1063 [11:25<12:26,  1.07it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape data for Brian Elmegaard: 'NoneType' object has no attribute 'find'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Professors:  26%|‚ñà‚ñà‚ñã       | 280/1063 [11:44<12:19,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape data for Salla Marjukka Laasonen: 'NoneType' object has no attribute 'find'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Professors:  32%|‚ñà‚ñà‚ñà‚ñè      | 338/1063 [12:57<12:22,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape data for Peter Bauer-Gottwein: 'NoneType' object has no attribute 'find'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Professors:  41%|‚ñà‚ñà‚ñà‚ñà      | 431/1063 [14:57<10:56,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape data for Ida Stub Johansson: 'NoneType' object has no attribute 'find'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Professors:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 604/1063 [18:28<07:06,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape data for Fengwen Wang: 'NoneType' object has no attribute 'find'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Professors:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 649/1063 [19:22<06:46,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape data for Babak Rezaei: 'NoneType' object has no attribute 'find'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Professors:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 809/1063 [22:41<04:33,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape data for Maria Ingeman: 'NoneType' object has no attribute 'find'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Professors:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 821/1063 [22:55<03:40,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape data for Mette Lode Skovbo: 'NoneType' object has no attribute 'find'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Professors:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 843/1063 [23:21<03:35,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape data for Markus Reinm√∂ller: 'NoneType' object has no attribute 'find'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Professors:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 969/1063 [25:56<01:24,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape data for J√∏rgen Henrik Klinge Jacobsen: 'NoneType' object has no attribute 'find'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Professors:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 993/1063 [26:26<01:15,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape data for Amelie Sina Wilde: 'NoneType' object has no attribute 'find'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Professors:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 1033/1063 [27:15<00:27,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape data for Stig Christian Herluf S Andersen: 'NoneType' object has no attribute 'find'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Professors:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1041/1063 [27:24<00:20,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape data for Rasmus Eckholdt Andersen: 'NoneType' object has no attribute 'find'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Professors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1063/1063 [27:53<00:00,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all professor data to data/all_professors.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scraper = DTUOrbitScraper()\n",
    "\n",
    "professor_columns = [\n",
    "    \"MAIN_RESPONSIBLE_NAME\", \"CO_RESPONSIBLE_1_NAME\",\n",
    "    \"CO_RESPONSIBLE_2_NAME\", \"CO_RESPONSIBLE_3_NAME\", \"CO_RESPONSIBLE_4_NAME\"\n",
    "]\n",
    "\n",
    "# Extract unique professors from the dataset\n",
    "def extract_professors(dataframe, professor_columns):\n",
    "    professors = set()\n",
    "    for col in professor_columns:\n",
    "        professors.update(dataframe[col].dropna().unique())\n",
    "    return list(professors)\n",
    "\n",
    "\n",
    "# Function to scrape professor data\n",
    "def scrape_professor_data(professors, output_file):\n",
    "    # if file already exists just cancel and return with a print statement\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"File {output_file} already exists. Skipping scraping.\")\n",
    "        return\n",
    "\n",
    "    all_data = {}\n",
    "    \n",
    "    for professor in tqdm(professors, desc=\"Scraping Professors\"):\n",
    "        # Skip if the professor already exists in the JSON file\n",
    "        if professor in all_data:\n",
    "            print(f\"Skipping {professor} as it already exists in the JSON file.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Use the scraper to get the profile data\n",
    "            profile_info = scraper.get_profile_info(professor)\n",
    "            all_data[professor] = profile_info  # Add to dictionary\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to scrape data for {professor}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(all_data, f, indent=4)\n",
    "    \n",
    "    print(f\"Saved all professor data to {output_file}\")\n",
    "\n",
    "\n",
    "output_file = \"data/all_professors.json\"\n",
    "professors = extract_professors(course_df, professor_columns)\n",
    "scrape_professor_data(professors, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define columns of interest for constructing the network\n",
    "professor_columns = [\n",
    "    \"MAIN_RESPONSIBLE_NAME\", \"CO_RESPONSIBLE_1_NAME\",\n",
    "    \"CO_RESPONSIBLE_2_NAME\", \"CO_RESPONSIBLE_3_NAME\", \"CO_RESPONSIBLE_4_NAME\"\n",
    "]\n",
    "\n",
    "course_columns = [\n",
    "    \"COURSE\", \"NAME\", \"ECTS_POINTS\", \"COURSE_TYPE\",\n",
    "    \"AVERAGE_GRADE\", \"PERCENT_PASSED\", \"PERCENT_FAILED\"\n",
    "]\n",
    "\n",
    "# Extract course attributes and professor-course relationships\n",
    "course_nodes = course_df[course_columns].drop_duplicates(subset=[\"COURSE\"]).set_index(\"COURSE\").to_dict(\"index\")\n",
    "\n",
    "edges = []\n",
    "for _, row in course_df.iterrows():\n",
    "    course_id = row[\"COURSE\"]\n",
    "    for professor_column in professor_columns:\n",
    "        professor_name = row[professor_column]\n",
    "        if pd.notna(professor_name):  # Ensure the name is valid\n",
    "            edges.append((course_id, professor_name))\n",
    "\n",
    "# Extract unique professor names for creating professor nodes\n",
    "professor_names = set([edge[1] for edge in edges])\n",
    "\n",
    "# Initialize a NetworkX graph\n",
    "B = nx.Graph()\n",
    "\n",
    "# Add course nodes with attributes\n",
    "for course_id, attributes in course_nodes.items():\n",
    "    B.add_node(course_id, **attributes, bipartite=0)\n",
    "\n",
    "# Add professor nodes without attributes for now (attributes will be added after scraping)\n",
    "for professor_name in professor_names:\n",
    "    B.add_node(professor_name, bipartite=1)\n",
    "\n",
    "# Add edges between courses and professors\n",
    "B.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_colors = [\n",
    "    \"skyblue\" if B.nodes[node].get(\"bipartite\") == 0 else \"lightgreen\"\n",
    "    for node in B.nodes\n",
    "]\n",
    "\n",
    "\n",
    "# Layout for bipartite graph (spring layout for aesthetics)\n",
    "pos = nx.spring_layout(B, k=0.3, iterations=50, seed=42)  # Adjusted for better spacing\n",
    "\n",
    "# Plot the network\n",
    "plt.figure(figsize=(15, 15))\n",
    "nx.draw(\n",
    "    B,\n",
    "    pos,\n",
    "    with_labels=False,  # Disable labels for a cleaner look\n",
    "    node_color=node_colors,\n",
    "    edge_color=\"gray\",\n",
    "    alpha=0.7,\n",
    "    linewidths=0.5\n",
    ")\n",
    "\n",
    "# Add legend\n",
    "handles = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='skyblue', markersize=10, label='Courses'),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightgreen', markersize=10, label='Professors')\n",
    "]\n",
    "plt.legend(handles=handles, loc=\"upper right\", fontsize=12, frameon=True)\n",
    "\n",
    "plt.title(\"Bipartite Network of Courses and Professors\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scholia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
